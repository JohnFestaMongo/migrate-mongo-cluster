# added wiredTigerCacheSizeGB to limit the overall system consumption
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet replset --dbpath source-cluster/data/replset/rs0/db --logpath source-cluster/data/replset/rs0/mongod.log --port 18000 --bind_ip 0.0.0.0 --logappend --fork
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet replset --dbpath source-cluster/data/replset/rs1/db --logpath source-cluster/data/replset/rs1/mongod.log --port 18001 --bind_ip 0.0.0.0 --logappend --fork
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet replset --dbpath source-cluster/data/replset/rs2/db --logpath source-cluster/data/replset/rs2/mongod.log --port 18002 --bind_ip 0.0.0.0 --logappend --fork

/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet replset --dbpath target-cluster/data/replset/rs0/db --logpath target-cluster/data/replset/rs0/mongod.log --port 18100 --bind_ip 0.0.0.0 --logappend --fork
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet replset --dbpath target-cluster/data/replset/rs1/db --logpath target-cluster/data/replset/rs1/mongod.log --port 18101 --bind_ip 0.0.0.0 --logappend --fork
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet replset --dbpath target-cluster/data/replset/rs2/db --logpath target-cluster/data/replset/rs2/mongod.log --port 18102 --bind_ip 0.0.0.0 --logappend --fork

/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet rsOplog --dbpath oplog-store/data/rsOplog/rs0/db --logpath oplog-store/data/rsOplog/rs0/mongod.log --port 18200 --bind_ip 0.0.0.0 --logappend --fork
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet rsOplog --dbpath oplog-store/data/rsOplog/rs1/db --logpath oplog-store/data/rsOplog/rs1/mongod.log --port 18201 --bind_ip 0.0.0.0 --logappend --fork
/opt/mongodb/v3.4.18/bin/mongod --wiredTigerCacheSizeGB 1 --replSet rsOplog --dbpath oplog-store/data/rsOplog/rs2/db --logpath oplog-store/data/rsOplog/rs2/mongod.log --port 18202 --bind_ip 0.0.0.0 --logappend --fork

mongo admin --eval 'db.shutdownServer({force: true})' --port 18000
mongo admin --eval 'db.shutdownServer({force: true})' --port 18001
mongo admin --eval 'db.shutdownServer({force: true})' --port 18002

mongo admin --eval 'db.shutdownServer({force: true})' --port 18100
mongo admin --eval 'db.shutdownServer({force: true})' --port 18101
mongo admin --eval 'db.shutdownServer({force: true})' --port 18102

mongo admin --eval 'db.shutdownServer({force: true})' --port 18200
mongo admin --eval 'db.shutdownServer({force: true})' --port 18201
mongo admin --eval 'db.shutdownServer({force: true})' --port 18202


# clean up
mongo --port 18200
db.getSiblingDB('test').dropDatabase();
db.getSiblingDB('migrate-mongo').dropDatabase();


// initial populate
var a,d;
for (a=1; a < 2100; a++) {
    var batch = []
    for (d=1; d <= a; d++) {
        var item = { accountId: a, deviceId: d, eventDate: ISODate("2019-01-11T03:00:00Z"), somedata: "Junk" };
        batch.push(item);
    }
    db.deviceState.insertMany(batch);
}


// real time update: slow add
var i = 0;
while(i < 1000) {
    i ++;
    sleep(10);
    var index = ''; //parseInt(Math.random()*5)+1;
    db.getCollection('deviceState' + index).insert({fname: 'fname'+index});
}


// real time update: fast add
var a,d;
for (a=2150; a < 2200; a++) {
    var batch = []
    for (d=1; d <= a; d++) {
        var item = { accountId: a, deviceId: d, eventDate: ISODate("2019-01-15T03:00:00Z"), somedata: "realtime Junk" };
        // batch.push(item);
        db.deviceState.insert(item);
    }
    // db.deviceState.insertMany(batch);
}




// real time update: faster add
var a,d;
for (a=2200; a < 2300; a++) {
    var batch = []
    for (d=1; d <= a; d++) {
        var item = { accountId: a, deviceId: d, eventDate: ISODate("2019-01-15T03:00:00Z"), somedata: "realtime Junk" };
        // batch.push(item);
        db.deviceState.insert(item);
    }
    // db.deviceState.insertMany(batch);
}

## Display the collections and their counts

var dbs=['test','exampledb'];
dbs.forEach(dbName => {
    var collections = db.getSiblingDB(dbName).getCollectionNames();
    collections.forEach(collName => print(`${dbName}.${collName} - ${db.getSiblingDB(dbName).getCollection(collName).countDocuments({})}`));
})


print(new Date() + ' - ' + db.getSiblingDB('test').getCollection('deviceState5').count())


# Test cases

- [x] preinsert, migrate
- [x] preinsert, migrate, wait, add slow, wait
- [x] preinsert, migrate, wait, stop/restart, (add slow, wait)
- [x] preinsert, migrate, wait, add slow, wait, (add fast, wait)
- [x] preinsert, migrate, wait, add fast, wait
- [x] preinsert, migrate, wait, add fast, wait, (add fast, wait)
- [x] preinsert, add slow, migrate, wait
- [x] preinsert, add fast, migrate, wait


# Next steps

- Bug I noticed some larger set on target
- Test with smaller size. 




// very simple database testing
var a,d;
for (a=1; a < 500; a++) {
    var batch = []
    for (d=1; d <= a; d++) {
        var item = { accountId: a, deviceId: d, eventDate: ISODate("2019-01-11T03:00:00Z"), somedata: "Junk" };
        batch.push(item);
    }
    db.deviceState.insertMany(batch);
}

db.getSiblingDB('local').getCollection('oplog.rs').find().sort({'$natural': -1}).limit(1).pretty()


data = db.deviceState.find().toArray();

for (var i = 0; i < data.length; i++) {
    var item = data[i];
    item._id = new ObjectId();
    bulk.insert(item);
}
var bulkResult = bulk.execute();


// bulk populate to lot more documents
var a,d;
for (a=1000; a < 1010; a++) {
    var bulk = db.deviceState.initializeUnorderedBulkOp();
    for (d=1; d <= 100000; d++) {
        var item = { accountId: a, deviceId: d, eventDate: ISODate("2019-01-11T03:00:00Z"), somedata: "Junk" };
        bulk.insert(item);
    }
    var bulkResult = bulk.execute();
    print("done with iteration " + a)
}


# run this on oplog
use migrate-mongo;
db.oplog.tracker.findOne()
Timestamp(1551654994, 57870)

# run this on source
db.getSiblingDB('local').getCollection('oplog.rs').count({ts: {$gte: Timestamp(1551661274, 1)}, op: {$ne : "n"} })

use test;
db.deviceState.findOne()


[INFO] 2019-03-03 19:21:12 INFO  [main] connection:71 - Opened connection [connectionId{localValue:77, serverValue:393}] to shyams-mac:18200
[WARN] 2019-03-03 19:21:12 WARN  [main] OplogWriter:114 - total models added 0 is not equal to operations injected 1000
[INFO] 2019-03-03 19:21:12 INFO  [Timer-0] OplogBufferedReader:98 - collectAndNotify invoked by [Elapsed Timer] is notifying subscribers about [0] documents. Total notified 1000
Exception in thread "Timer-0" java.lang.NullPointerException
	at io.reactivex.internal.observers.LambdaObserver.onNext(LambdaObserver.java:66)
	at com.mongodb.migratecluster.observables.OplogBufferedReader.collectAndNotify(OplogBufferedReader.java:103)
	at com.mongodb.migratecluster.observables.OplogBufferedReader.lambda$subscribeActual$0(OplogBufferedReader.java:68)
	at com.mongodb.migratecluster.utils.Timer$1.run(Timer.java:39)
	at java.base/java.util.TimerThread.mainLoop(Timer.java:556)
	at java.base/java.util.TimerThread.run(Timer.java:506)
[INFO] 2019-03-03 19:21:14 INFO  [RxCachedThreadScheduler-2] OplogMigrator:179 - Target is behind by 1 seconds & 0000 operations; Target: Timestamp{value=6664336990395172287, seconds=1551661871, inc=1471}, Source: Timestamp{value=6664336994690138834, seconds=1551661872, inc=722}
[INFO] 2019-03-03 19:21:19 INFO  [RxCachedThreadScheduler-2] OplogMigrator:179 - Target is behind by 1 seconds & 0000 operations; Target: Timestamp{value=6664336990395172287, seconds=1551661871, inc=1471}, Source: Timestamp{value=6664336994690138834, seconds=1551661872, inc=722}
[INFO] 2019-03-03 19:21:24 INFO  [RxCachedThreadScheduler-2] OplogMigrator:179 - Target is behind by 1 seconds & 0000 operations; Target: Timestamp{value=6664336990395172287, seconds=1551661871, inc=1471}, Source: Timestamp{value=6664336994690138834, seconds=1551661872, inc=722}


## Adhoc fixes how to

use migrate-mongo;
db.oplog.tracker.findOne()
{
	"_id" : ObjectId("5c7d6831c7481f3d76f1d6be"),
	"reader" : "localhost:18000,localhost:18001,localhost:18002/?replicaSet=replset",
	"ts" : Timestamp(1551722540, 1)
}

use test;
db.deviceState.find().pretty()
{
	"_id" : ObjectId("5c7d7e69c7481f3d760c70be"),
	"collection" : "deviceState",
	"database" : "test",
	"reader" : "localhost:18000,localhost:18001,localhost:18002/?replicaSet=replset",
	"latest_id" : ObjectId("5c79bb21ce613cf77150f923")
}
{
	"_id" : ObjectId("5c7d7e69c7481f3d760c70bd"),
	"collection" : "deviceState",
	"database" : "test",
	"reader" : "localhost:18000,localhost:18001,localhost:18002/?replicaSet=replset",
	"latest_id" : ObjectId("5c7c02210565c8efdc5da6f7")
}

### if stopped at record count and you want to revive from there

// find the next record to resume from 
db.deviceState.find({},{_id: 1}).sort({_id: 1}).skip(88907447).limit(1)
{ "_id" : ObjectId("5c7b0389da9fc070bd08f12f") }

// find documents that are greater than the id
db.deviceState.count({_id: {$gt : ObjectId("5c79bb21ce613cf77150fd0b")}},{_id: 1})
db.deviceState.countDocuments({_id: {$gt : ObjectId("5c7b0389da9fc070bd08f12f")}})
// 102668075

db.deviceState.count({_id: {$gt : ObjectId("5c7b0389da9fc070bd08f12f")}},{_id: 1})
db.deviceState.countDocuments({_id: {$gt : ObjectId("5c7b0389da9fc070bd08f12f")}})
// 13762627 yet to sync
